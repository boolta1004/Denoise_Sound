{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Foldername : 0 - 20 files\n",
      "Foldername : 1 - 20 files\n",
      "Foldername : 2 - 20 files\n",
      "Foldername : 3 - 20 files\n",
      "Foldername : 4 - 20 files\n",
      "Foldername : 5 - 20 files\n",
      "X_data : (60240, 13)\n",
      "Y_label : (60240, 6)\n",
      "6 classes!!\n",
      "X_train : (45180, 13)\n",
      "Y_train : (45180, 6)\n",
      "X_test : (15060, 13)\n",
      "Y_test : (15060, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import librosa\n",
    "import pyaudio\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import os\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = int(RATE / 10)\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "DATA_PATH = \"./data/\"\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "def load_wave_generator(path):    #음성 데이터 npy로 변경\n",
    "\n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []\n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "\n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue \n",
    "        files = os.listdir(path+\"/\"+folder)\n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"files\")\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "\n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "\n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "#t = np.array(X_train);\n",
    "#print(\"!!!!!!!!\",t,t.shape,X_train)\n",
    "print(tf_classes,\"classes!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "####################\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X_train, Y_train)\n",
    "####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 2.064874768\n",
      "Epoch: 0001 cost = 1.868339419\n",
      "Epoch: 0002 cost = 1.838817835\n",
      "Epoch: 0003 cost = 1.824440539\n",
      "Epoch: 0004 cost = 1.817013144\n",
      "Epoch: 0005 cost = 1.811315775\n",
      "Epoch: 0006 cost = 1.807290137\n",
      "Epoch: 0007 cost = 1.802938223\n",
      "Epoch: 0008 cost = 1.800034583\n",
      "Epoch: 0009 cost = 1.797730863\n",
      "Epoch: 0010 cost = 1.796716869\n",
      "Epoch: 0011 cost = 1.791337371\n",
      "Epoch: 0012 cost = 1.785406113\n",
      "Epoch: 0013 cost = 1.779751837\n",
      "Epoch: 0014 cost = 1.768437564\n",
      "Epoch: 0015 cost = 1.756364286\n",
      "Epoch: 0016 cost = 1.741080761\n",
      "Epoch: 0017 cost = 1.723903716\n",
      "Epoch: 0018 cost = 1.703667283\n",
      "Epoch: 0019 cost = 1.683926702\n",
      "Epoch: 0020 cost = 1.665920258\n",
      "Epoch: 0021 cost = 1.648683906\n",
      "Epoch: 0022 cost = 1.629659593\n",
      "Epoch: 0023 cost = 1.611580372\n",
      "Epoch: 0024 cost = 1.599230587\n",
      "Epoch: 0025 cost = 1.583233297\n",
      "Epoch: 0026 cost = 1.561247647\n",
      "Epoch: 0027 cost = 1.547543406\n",
      "Epoch: 0028 cost = 1.529516578\n",
      "Epoch: 0029 cost = 1.515975118\n",
      "Epoch: 0030 cost = 1.496094346\n",
      "Epoch: 0031 cost = 1.479024708\n",
      "Epoch: 0032 cost = 1.454778254\n",
      "Epoch: 0033 cost = 1.432886541\n",
      "Epoch: 0034 cost = 1.408460140\n",
      "Epoch: 0035 cost = 1.396229684\n",
      "Epoch: 0036 cost = 1.371248543\n",
      "Epoch: 0037 cost = 1.353725076\n",
      "Epoch: 0038 cost = 1.330227137\n",
      "Epoch: 0039 cost = 1.314672053\n",
      "Epoch: 0040 cost = 1.295721769\n",
      "Epoch: 0041 cost = 1.282152832\n",
      "Epoch: 0042 cost = 1.265000165\n",
      "Epoch: 0043 cost = 1.250376821\n",
      "Epoch: 0044 cost = 1.241168261\n",
      "Epoch: 0045 cost = 1.219924867\n",
      "Epoch: 0046 cost = 1.211714864\n",
      "Epoch: 0047 cost = 1.194792211\n",
      "Epoch: 0048 cost = 1.190999627\n",
      "Epoch: 0049 cost = 1.179573059\n",
      "Epoch: 0050 cost = 1.173570335\n",
      "Epoch: 0051 cost = 1.161756992\n",
      "Epoch: 0052 cost = 1.151751637\n",
      "Epoch: 0053 cost = 1.144164383\n",
      "Epoch: 0054 cost = 1.139970064\n",
      "Epoch: 0055 cost = 1.133811831\n",
      "Epoch: 0056 cost = 1.127516627\n",
      "Epoch: 0057 cost = 1.125279725\n",
      "Epoch: 0058 cost = 1.115695715\n",
      "Epoch: 0059 cost = 1.107730806\n",
      "Epoch: 0060 cost = 1.100110352\n",
      "Epoch: 0061 cost = 1.095566154\n",
      "Epoch: 0062 cost = 1.086967051\n",
      "Epoch: 0063 cost = 1.087068379\n",
      "Epoch: 0064 cost = 1.079714537\n",
      "Epoch: 0065 cost = 1.071469128\n",
      "Epoch: 0066 cost = 1.067855597\n",
      "Epoch: 0067 cost = 1.066646039\n",
      "Epoch: 0068 cost = 1.058341205\n",
      "Epoch: 0069 cost = 1.051995397\n",
      "Epoch: 0070 cost = 1.052906156\n",
      "Epoch: 0071 cost = 1.044393063\n",
      "Epoch: 0072 cost = 1.040135622\n",
      "Epoch: 0073 cost = 1.036006570\n",
      "Epoch: 0074 cost = 1.032801986\n",
      "Epoch: 0075 cost = 1.028470516\n",
      "Epoch: 0076 cost = 1.025360525\n",
      "Epoch: 0077 cost = 1.022539496\n",
      "Epoch: 0078 cost = 1.014722049\n",
      "Epoch: 0079 cost = 1.014090002\n",
      "Epoch: 0080 cost = 1.014045775\n",
      "Epoch: 0081 cost = 1.004157603\n",
      "Epoch: 0082 cost = 1.002934813\n",
      "Epoch: 0083 cost = 0.998138458\n",
      "Epoch: 0084 cost = 0.996915311\n",
      "Epoch: 0085 cost = 0.989661902\n",
      "Epoch: 0086 cost = 0.990853131\n",
      "Epoch: 0087 cost = 0.983703464\n",
      "Epoch: 0088 cost = 0.980331451\n",
      "Epoch: 0089 cost = 0.973802656\n",
      "Epoch: 0090 cost = 0.969991624\n",
      "Epoch: 0091 cost = 0.963122815\n",
      "Epoch: 0092 cost = 0.963278592\n",
      "Epoch: 0093 cost = 0.958452553\n",
      "Epoch: 0094 cost = 0.955273718\n",
      "Epoch: 0095 cost = 0.950405270\n",
      "Epoch: 0096 cost = 0.948535234\n",
      "Epoch: 0097 cost = 0.939756244\n",
      "Epoch: 0098 cost = 0.935054839\n",
      "Epoch: 0099 cost = 0.935292035\n",
      "Accuracy:  0.6935591\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\", allow_pickle=True)#allow_pickle add\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(13) \n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, 13])\n",
    "#\n",
    "Y = tf.compat.v1.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([216, 200]))\n",
    "# b = tf.Variable(tf.random_normal([200]))\n",
    "\n",
    "#1st hidden layer\n",
    "W1 = tf.compat.v1.get_variable(\"w1\",\n",
    "    #tf.random_normal([216, 180], mean=0, stddev=sd),\n",
    "        shape=[13, 256],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b1 = tf.Variable(tf.random.normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) \n",
    "L1 = tf.nn.dropout(L1, rate = 1 - (keep_prob))\n",
    "\n",
    "# 2nd hidden layer\n",
    "W2 = tf.compat.v1.get_variable(\"w2\",\n",
    "    #tf.random_normal([180, 150], mean=0, stddev=sd),\n",
    "         shape=[256, 256],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b2 = tf.Variable(tf.random.normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, rate = 1 - (keep_prob))\n",
    "\n",
    "# 3th hidden layer\n",
    "W3 = tf.compat.v1.get_variable(\"w3\",\n",
    "    #tf.random_normal([150, 100], mean=0, stddev=sd),\n",
    "            shape=[256, 256],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b3 = tf.Variable(tf.random.normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, rate = 1 - (keep_prob))\n",
    "\n",
    "# 4th hidden layer\n",
    "W4 = tf.compat.v1.get_variable(\"w4\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[256, 128],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b4 = tf.Variable(tf.random.normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, rate = 1 - (keep_prob))\n",
    "\n",
    "# 5th hidden layer\n",
    "W5 = tf.compat.v1.get_variable(\"w5\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b5 = tf.Variable(tf.random.normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5)\n",
    "L5 = tf.nn.dropout(L5, rate = 1 - (keep_prob))\n",
    "\n",
    "# 6 hidden layer\n",
    "W6 = tf.compat.v1.get_variable(\"w6\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b6 = tf.Variable(tf.random.normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) \n",
    "L6 = tf.nn.dropout(L6, rate = 1 - (keep_prob))\n",
    "\n",
    "# 7th hidden layer\n",
    "W7 = tf.compat.v1.get_variable(\"w7\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b7 = tf.Variable(tf.random.normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7)\n",
    "L7 = tf.nn.dropout(L7, rate = 1 - (keep_prob))\n",
    "\n",
    "# final layer\n",
    "W8 = tf.compat.v1.get_variable(\"w8\",\n",
    "    #tf.random_normal([50, tf_classes], mean=0, stddev=sd),\n",
    "            shape=[128, tf_classes],\n",
    "                     initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "b8 = tf.Variable(tf.random.normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=tf.stop_gradient(Y)))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "#*2\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.934049457\n",
      "Epoch: 0001 cost = 0.924213469\n",
      "Epoch: 0002 cost = 0.920375049\n",
      "Epoch: 0003 cost = 0.916062355\n",
      "Epoch: 0004 cost = 0.907304972\n",
      "Epoch: 0005 cost = 0.908597350\n",
      "Epoch: 0006 cost = 0.906613648\n",
      "Epoch: 0007 cost = 0.901054263\n",
      "Epoch: 0008 cost = 0.892286867\n",
      "Epoch: 0009 cost = 0.890569240\n",
      "Epoch: 0010 cost = 0.887806118\n",
      "Epoch: 0011 cost = 0.882237166\n",
      "Epoch: 0012 cost = 0.877040356\n",
      "Epoch: 0013 cost = 0.871224612\n",
      "Epoch: 0014 cost = 0.869587451\n",
      "Epoch: 0015 cost = 0.869379252\n",
      "Epoch: 0016 cost = 0.863109887\n",
      "Epoch: 0017 cost = 0.855811030\n",
      "Epoch: 0018 cost = 0.857121617\n",
      "Epoch: 0019 cost = 0.856333256\n",
      "Epoch: 0020 cost = 0.851510882\n",
      "Epoch: 0021 cost = 0.850098401\n",
      "Epoch: 0022 cost = 0.847107917\n",
      "Epoch: 0023 cost = 0.840172261\n",
      "Epoch: 0024 cost = 0.839311540\n",
      "Epoch: 0025 cost = 0.837675035\n",
      "Epoch: 0026 cost = 0.834271103\n",
      "Epoch: 0027 cost = 0.831058562\n",
      "Epoch: 0028 cost = 0.824958831\n",
      "Epoch: 0029 cost = 0.823845744\n",
      "Epoch: 0030 cost = 0.820797205\n",
      "Epoch: 0031 cost = 0.814559281\n",
      "Epoch: 0032 cost = 0.810815394\n",
      "Epoch: 0033 cost = 0.810978085\n",
      "Epoch: 0034 cost = 0.803358346\n",
      "Epoch: 0035 cost = 0.798414916\n",
      "Epoch: 0036 cost = 0.800953984\n",
      "Epoch: 0037 cost = 0.798030794\n",
      "Epoch: 0038 cost = 0.795734644\n",
      "Epoch: 0039 cost = 0.788200647\n",
      "Epoch: 0040 cost = 0.788143009\n",
      "Epoch: 0041 cost = 0.781871378\n",
      "Epoch: 0042 cost = 0.779850274\n",
      "Epoch: 0043 cost = 0.781569898\n",
      "Epoch: 0044 cost = 0.776659071\n",
      "Epoch: 0045 cost = 0.775062859\n",
      "Epoch: 0046 cost = 0.771653056\n",
      "Epoch: 0047 cost = 0.766998321\n",
      "Epoch: 0048 cost = 0.767936051\n",
      "Epoch: 0049 cost = 0.767901152\n",
      "Epoch: 0050 cost = 0.766189575\n",
      "Epoch: 0051 cost = 0.760082275\n",
      "Epoch: 0052 cost = 0.757389247\n",
      "Epoch: 0053 cost = 0.754722536\n",
      "Epoch: 0054 cost = 0.751990139\n",
      "Epoch: 0055 cost = 0.749485970\n",
      "Epoch: 0056 cost = 0.745948583\n",
      "Epoch: 0057 cost = 0.744374990\n",
      "Epoch: 0058 cost = 0.742200315\n",
      "Epoch: 0059 cost = 0.743523031\n",
      "Epoch: 0060 cost = 0.733274341\n",
      "Epoch: 0061 cost = 0.735428572\n",
      "Epoch: 0062 cost = 0.735424191\n",
      "Epoch: 0063 cost = 0.728064626\n",
      "Epoch: 0064 cost = 0.726463169\n",
      "Epoch: 0065 cost = 0.728465408\n",
      "Epoch: 0066 cost = 0.721554667\n",
      "Epoch: 0067 cost = 0.723398268\n",
      "Epoch: 0068 cost = 0.721890032\n",
      "Epoch: 0069 cost = 0.716508090\n",
      "Epoch: 0070 cost = 0.715553731\n",
      "Epoch: 0071 cost = 0.716569960\n",
      "Epoch: 0072 cost = 0.714628577\n",
      "Epoch: 0073 cost = 0.709800780\n",
      "Epoch: 0074 cost = 0.709464610\n",
      "Epoch: 0075 cost = 0.708502799\n",
      "Epoch: 0076 cost = 0.701944381\n",
      "Epoch: 0077 cost = 0.702866852\n",
      "Epoch: 0078 cost = 0.698850095\n",
      "Epoch: 0079 cost = 0.700665474\n",
      "Epoch: 0080 cost = 0.695576727\n",
      "Epoch: 0081 cost = 0.695446223\n",
      "Epoch: 0082 cost = 0.695057690\n",
      "Epoch: 0083 cost = 0.694280982\n",
      "Epoch: 0084 cost = 0.693118453\n",
      "Epoch: 0085 cost = 0.693365991\n",
      "Epoch: 0086 cost = 0.690079451\n",
      "Epoch: 0087 cost = 0.689347148\n",
      "Epoch: 0088 cost = 0.686168909\n",
      "Epoch: 0089 cost = 0.681217849\n",
      "Epoch: 0090 cost = 0.681531757\n",
      "Epoch: 0091 cost = 0.682494640\n",
      "Epoch: 0092 cost = 0.681112856\n",
      "Epoch: 0093 cost = 0.678622544\n",
      "Epoch: 0094 cost = 0.673243165\n",
      "Epoch: 0095 cost = 0.675714940\n",
      "Epoch: 0096 cost = 0.671759188\n",
      "Epoch: 0097 cost = 0.674589872\n",
      "Epoch: 0098 cost = 0.674564809\n",
      "Epoch: 0099 cost = 0.669828504\n",
      "Accuracy:  0.81713146\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_voice_model2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 13)\n",
      "(502, 6)\n",
      "predict\n",
      "2    488\n",
      "1      4\n",
      "3      4\n",
      "5      4\n",
      "4      2\n",
      "dtype: int64\n",
      "Accuracy:  0.9721116\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_이재은.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 유인나\n",
    "1 배철수\n",
    "2 이재은\n",
    "3 최일구\n",
    "4 문재인 대통령\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[2] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(input=hypothesis, axis=1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 13)\n",
      "(502, 6)\n",
      "predict\n",
      "4    354\n",
      "3     42\n",
      "0     37\n",
      "2     31\n",
      "1     29\n",
      "5      9\n",
      "dtype: int64\n",
      "Accuracy:  0.7051793\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_문재인대통령.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 유인나\n",
    "1 배철수\n",
    "2 이재은\n",
    "3 최일구\n",
    "4 문재인 대통령\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[4] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(input=hypothesis, axis=1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 13)\n",
      "(502, 6)\n",
      "predict\n",
      "0    468\n",
      "2     15\n",
      "4     13\n",
      "1      5\n",
      "3      1\n",
      "dtype: int64\n",
      "Accuracy:  0.93227094\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_유인나1.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 유인나\n",
    "1 배철수\n",
    "2 이재은\n",
    "3 최일구\n",
    "4 문재인 대통령\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[0] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(input=hypothesis, axis=1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 13)\n",
      "(502, 6)\n",
      "predict\n",
      "0    441\n",
      "5     37\n",
      "1     12\n",
      "3      9\n",
      "2      2\n",
      "4      1\n",
      "dtype: int64\n",
      "Accuracy:  0.87848604\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_유인나2.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 유인나\n",
    "1 배철수\n",
    "2 이재은\n",
    "3 최일구\n",
    "4 문재인 대통령\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[0] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(input=hypothesis, axis=1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 13)\n",
      "(502, 6)\n",
      "predict\n",
      "1    416\n",
      "3     26\n",
      "2     20\n",
      "4     20\n",
      "0     14\n",
      "5      6\n",
      "dtype: int64\n",
      "Accuracy:  0.8286853\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_배철수.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 유인나\n",
    "1 배철수\n",
    "2 이재은\n",
    "3 최일구\n",
    "4 문재인 대통령\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[1] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(input=hypothesis, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(input=hypothesis, axis=1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
